{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T17:56:20.918649Z",
     "start_time": "2022-02-21T17:56:20.913603Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T17:56:21.289325Z",
     "start_time": "2022-02-21T17:56:21.282239Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available: True\n",
      "Current device: 0\n",
      "Device: cuda:0\n",
      "Device count: 1\n",
      "Device name: GeForce GTX 1070\n"
     ]
    }
   ],
   "source": [
    "available = torch.cuda.is_available()\n",
    "curr_device = torch.cuda.current_device()\n",
    "device = torch.device(\"cuda:0\" if available else \"cpu\")\n",
    "device_count = torch.cuda.device_count() \n",
    "device_name =  torch.cuda.get_device_name(0)\n",
    "\n",
    "print(f'Cuda available: {available}')\n",
    "print(f'Current device: {curr_device}')\n",
    "print(f'Device: {device}')\n",
    "print(f'Device count: {device_count}')\n",
    "print(f'Device name: {device_name}')\n",
    "\n",
    "#device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-27T01:28:18.337438Z",
     "start_time": "2022-02-27T01:28:18.323099Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyBatchNorm2d(nn.modules.batchnorm._NormBase):\n",
    "    ''' Partially based on: \n",
    "        https://pytorch.org/docs/stable/_modules/torch/nn/modules/batchnorm.html#BatchNorm2d\n",
    "        https://discuss.pytorch.org/t/implementing-batchnorm-in-pytorch-problem-with-updating-self-running-mean-and-self-running-var/49314/5 \n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features,\n",
    "        eps=1e-5,\n",
    "        momentum=0.1,\n",
    "        device=None,\n",
    "        dtype=None\n",
    "    ):\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype, 'affine': False, 'track_running_stats': True}\n",
    "        super(MyBatchNorm2d, self).__init__(\n",
    "            num_features, eps, momentum, **factory_kwargs\n",
    "        )\n",
    "        \n",
    "    def _check_input_dim(self, input):\n",
    "        if input.dim() != 4:\n",
    "            raise ValueError(\"expected 4D input (got {}D input)\".format(input.dim()))\n",
    "\n",
    "    def forward(self, input):\n",
    "        self._check_input_dim(input)\n",
    "    \n",
    "        if self.training:\n",
    "            var, mean = torch.var_mean(input, [0, 2, 3], unbiased=False) # along channel axis\n",
    "            unbiased_var = torch.var(input, [0, 2, 3], unbiased=True) # along channel axis\n",
    "            self.running_mean = (1.0 - self.momentum) * self.running_mean + self.momentum * mean\n",
    "            \n",
    "            # Strange: PyTorch impl. of running variance uses biased_variance for the batch calc but\n",
    "            # *unbiased_var* for the running_var!\n",
    "            # https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/Normalization.cpp#L190\n",
    "            self.running_var = (1.0 - self.momentum) * self.running_var + self.momentum * unbiased_var\n",
    "        else:\n",
    "            mean = self.running_mean\n",
    "            var = self.running_var\n",
    "\n",
    "        # change shape\n",
    "        current_mean = mean.view([1, self.num_features, 1, 1]).expand_as(input)\n",
    "        current_var = var.view([1, self.num_features, 1, 1]).expand_as(input)\n",
    "\n",
    "        # get output\n",
    "        denom = (current_var + self.eps)\n",
    "        y = (input - current_mean) / denom.sqrt()\n",
    "\n",
    "        return y, denom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-27T01:28:18.515315Z",
     "start_time": "2022-02-27T01:28:18.491135Z"
    }
   },
   "outputs": [],
   "source": [
    "eps = 1e-8\n",
    "for i in range(10):\n",
    "    input = torch.randn(2, 3, 2, 2)\n",
    "    \n",
    "    # Without Learnable Parameters\n",
    "    m = nn.BatchNorm2d(3, affine=False, momentum=0.5)\n",
    "    output = m(input)\n",
    "    \n",
    "    m2 = MyBatchNorm2d(3, momentum=0.5)\n",
    "    output2, _ = m2(input)\n",
    "    \n",
    "    assert torch.any(torch.abs(output - output2) < eps), (i, output, output2)\n",
    "    assert torch.any(torch.abs(m.running_mean - m2.running_mean) < eps), (i, m.running_mean, m2.running_mean)\n",
    "    assert torch.any(torch.abs(m.running_var - m2.running_var) < eps), (i, m.running_var, m2.running_var)\n",
    "    \n",
    "    \n",
    "for i in range(10):\n",
    "    input = torch.randn(2, 3, 2, 2)\n",
    "    \n",
    "    # Without Learnable Parameters\n",
    "    m.eval()\n",
    "    output = m(input)\n",
    "    \n",
    "    m2.eval()\n",
    "    output2, _ = m2(input)\n",
    "    \n",
    "    assert torch.any(torch.abs(output - output2) < eps), (i, output, output2)\n",
    "    assert torch.any(torch.abs(m.running_mean - m2.running_mean) < eps), (i, m.running_mean, m2.running_mean)\n",
    "    assert torch.any(torch.abs(m.running_var - m2.running_var) < eps), (i, m.running_var, m2.running_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2022-02-21\n",
    "\n",
    "* Strange: PyTorch impl. of running variance uses biased variance for the batch calc but\n",
    "  *unbiased variance* for the running_var!\n",
    "* https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/Normalization.cpp#L190\n",
    "* Anyways, used their method of estimating it to get 1e-8 matching precision with their implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
