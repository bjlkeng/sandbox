# 2024-09-06
* Last time added conjugate gradient descent
* Added a Nesterov, RMSProp, Adam, Hypergradient Descent
* CoPilot isn't bad, when I started to write the Adam function, it filled it in entirely right away!  Most likely because it appears on the web so often.
* However, when it tried to do it from the hypergradient (probably a lot less common), it just basically copied what I had for Nesterov momentum.  Very telling in what the LLM has seen before.

# 2024-08-29
* Using ChatGPT to gain intuition about positive definite, second order multivariate approximation, why Hessian is analagous to second derivative
* https://chatgpt.com/share/41a66828-1b06-485e-80b0-244d92528f0f